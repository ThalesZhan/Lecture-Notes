\chapter{Stochastic Integral}

\section{Local Martingale}

\begin{defn}[Local Martingale]
    An $\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}$ adapted stochastic process $X=(X_t)_{t \geq 0}$ is called a local martingale if there exists a sequence stopping times $\bb{T_n}_{n \in \N}$ with $T_n \uparrow \infty$ such that $(X_{t \wedge T_n} \mathbb{I}_{\bb{T_n > 0}})_{t \geq 0}$ is a UI martingale w.s.t. $\mathbb{F}$.
\end{defn}
\begin{rmk}
    If $X_0 = 0$, we only need to require $(X_{t \wedge T_n})_{t \geq 0}$ is a UI martingale w.s.t. $\mathbb{F}$. Let $T_n = n$, clearly martingales are local martingales but the converse is not true.
\end{rmk}

\noindent \textbf{Notation:} Let $\mathcal{M}^{loc}$ be the set of all local martingales and $\mathcal{M}^{loc}_0 \subset \mathcal{M}^{loc}$ be the set of all local martingales with $X_0 = 0$.

\begin{lem}
    A local martingale with
    \begin{equation*}
    	\sup_{s \leq t}\abs{X_s} \in L^1
    \end{equation*}
    for any $t$ is a martingale.
\end{lem}
\begin{proof}
    Assume $X_0 = 0$. Let $T_n \uparrow \infty$ be stopping times such that $(X_{t \wedge T_n})_{t \geq 0}$ be UI martingales. Then for $s \leq t$,
    \begin{equation*}
    	\E\bj{X_{t \wedge T_n} \mid \mathcal{F}_s} = X_{s \wedge T_n}
    \end{equation*}
    By Martingale Convergence Theorem, as $n \sto \infty$,
    \begin{equation*}
    	X_{t \wedge T_n} \sto X_t,\quad X_{s \wedge T_n} \sto X_s
    \end{equation*}
    a.e.. Because
    \begin{equation*}
    	\sup _n\left|X_{t \wedge T_n}\right| \leq \sup _{0 \leq r \leq t}\left|X_r\right| \in L^1,
    \end{equation*}
    by DCT,
    \begin{equation*}
    	\E\bj{X_{t} \mid \mathcal{F}_s} = X_{s}.
    \end{equation*}
\end{proof}
\begin{rmk}
    In fact, from the proof, it is not difficult to see a local martingale $X=(X_t)_{t \geq 0}$ is a martingale if
    \begin{equation*}
    	\bb{X_T \colon T \in \mathcal{S}_t},
    \end{equation*}
    is UI for all $t$, where $\mathcal{S}_t \defeq \bb{T \text{ stopping time } \colon T \leq t}$. Such condition is called class (DL).
\end{rmk}

\begin{prop}
    Every nonnegative local martingale is a supermartingale.
\end{prop}
\begin{proof}
    Assume $X_0 = 0$. Let $T_n \uparrow \infty$ be stopping times such that $(X_{t \wedge T_n})_{t \geq 0}$ be UI martingales. Then for $s \leq t$,
    \begin{align*}
		X_s & =\lim _{n \rightarrow \infty} X_{s \wedge T_n}=\lim _{n \rightarrow \infty} \mathbb{E}\left[X_{t \wedge T_n} \mid \mathcal{F}_s\right] \\
		& \geq \mathbb{E}\left[\lim _{n \rightarrow \infty} X_{t \wedge T_n} \mid \mathcal{F}_s\right]=\mathbb{E}\left[X_t \mid \mathcal{F}_s\right]
    \end{align*}
    by Fatou's lemma.
\end{proof}

\begin{defn}
    Let $\bar{\Omega} = [0,\infty) \times \Omega$. 
    \begin{enumerate}[label=(\arabic{*})]
    	\item A $\sigma$-field $\mathcal{P}$ over $\bar{\Omega}$ is called predictable if it is generated by all left-continuous and adapted process $X \colon [0,\infty) \times \Omega \sto \R$.
    	\item A stochastic process $X=(X_t)_{t \geq 0}$ is called predictable if $X$ is $\mathcal{P}$-measurable on $\bar{\Omega}$.
    \end{enumerate}
\end{defn}
\begin{rmk}
    \begin{enumerate}[label=(\roman*)]
    	\item Every predictable process is adapted.
    	\item Every continuous and increasing process is predictable.
    	\item If filtration $\mathbb{F}$ satisfies the usual condition, every predictable process is adapted to $\mathbb{F}_- = (\mathcal{F}_{t-})_{t \geq 0 }$.
    \end{enumerate}
\end{rmk}

\noindent \textbf{Notation:} Let $\mathcal{M}^2$ be the set of all c\`adl\`ag martingales $X=(X_t)_{t \geq 0}$ such that
\begin{equation*}
	\sup_{t \geq 0} \E\bj{X_t^2} < \infty.
\end{equation*}
Let $\mathcal{M}_0^2 \subset \mathcal{M}^2$ be all $X \in \mathcal{M}^2$ with $X_0 = 0$. Let $\mathcal{M}^{2,c}_0 \subset \mathcal{M}_0^2$ be all $X \in \mathcal{M}^2$ that is continuous.
\begin{rmk}
    Note that if $M \in \mathcal{M}^2$, then $M$ is UI. So by convergence theorem, $M_\infty = \lim_t M_t$ in $L^1$ and $M_t = \E\bj{M_\infty \mid \mathcal{F}_t}$.
\end{rmk}

\begin{thm}[Doob-Meyer Decomposition]
    Let $X=(X)_{t \geq 0}$ be a right-continuous supermartingale. Assume 
    \begin{equation*}
    	\bb{X_T \colon T \in \mathcal{S}},
    \end{equation*}
    is UI, where $\mathcal{S} \defeq \bb{T \text{ stopping time } \colon T < \infty}$ ($X$ is called of class (D)). Then $X$ admits a unique decomposition
    \begin{equation*}
    	X_t = X_0 + M_t - A_t,
    \end{equation*}
    where $M$ is a right-continuous UI martingale with $M_0 = 0$ and $A$ is a increasing and right-continuous and predictable process with $A_0 = 0$.
\end{thm} 
\begin{rmk}
    Note that any c\`adl\`ag martingale is of class (DL), but a c\`adl\`ag martingale is of class (D) if and only if it is UI. These results are also true for a c\`adl\`ag positive submartingale.
\end{rmk}

\begin{cor}
    Let $M \in \mathcal{M}^2$. Then there exists a unique right-continuous predictable process $\inn{M} = (\inn{M}_t)_{t \geq 0}$ with $\inn{M} = 0$ such that $M^2 - \inn{M}$ is a martingale.
\end{cor}
\begin{proof}
    $M$ is a martingale so $-M^2$ is a supermartingale. To show of class (D), it suffices to show the UI of $(M_t^2)_{t \geq 0}$ because it is a positive submartingale. Because $M \in M^2$, $M$ is UI so $M_\infty = \lim_t M_t$ in $L^1$. So $M^2_\infty = \lim_t M_t^2$ in $L^1$, which implies that $M^2$ is UI. Then by Doob-Meyer decomposition.
    \begin{equation*}
    	-M^2 = \text{martingale } - \inn{M}. \qedhere
    \end{equation*}
\end{proof}
\begin{rmk}
    Because $\inn{M}$ is uniquely determined by $M$, it is called the quadratic variation of $M$. And $\sup_t \E[\inn{M}_t] = \E[M_0^2] < \infty$.
\end{rmk}

\begin{lem}
    Let $M \in M^{2,c}$. For partition $\Pi$ of $[0,t]$, we have
    \begin{equation*}
    	\lim_{\norm{\Pi} \sto 0} \sum_{t_k \in \Pi} \abs{M_{t_k} - M_{t_{k-1}}}^2 = \inn{M}_t
    \end{equation*}
    in probability.
\end{lem}
\begin{rmk}
    In fact, it is the general definition of quadratic variation.
\end{rmk}

\begin{defn}
    For $M,N \in \mathcal{M}^2$, the process
    \begin{equation*}
    	\inn{M,N} = \frac{1}{4}\bc{\inn{M+N}- \inn{M-N}}.
    \end{equation*}
    is called the cross variation (or quadratic covariation) of $M,N$.
\end{defn}
\begin{rmk}
    \begin{enumerate}[label=(\roman*)]
    	\item Note that $\inn{M,M}_t = \inn{M}_t$.
    	\item By definition, it is not hard to see $MN - \inn{M,N}$ is a martingale.
    	\item If $M,N \in M^{2,loc}$ right-continuous, then there exists a unique increasing right-continuous predictable process $\inn{M}$ and there exists a unique increasing right-continuous predictable process $\inn{M,N}$ of bounded variation such that
    	\begin{equation*}
    		M^2- \inn{M},\quad MN - \inn{M,N},
    	\end{equation*}
    	are local martingales.
    	\item Moreover,
    	\begin{equation*}
    		\lim _{\|\Pi\| \rightarrow 0} \sum_{k=1}^m\left(M_{t_k}-M_{t_{k-1}}\right)\left(N_{t_k}-N_{t_{k-1}}\right)=\langle M, N\rangle_t.
    	\end{equation*}
    \end{enumerate}
\end{rmk}

\begin{exam}[Variation of Brownian Motion]
    Given a Brownian motion $B=(B_t)_{t \geq 0}$, there are two ways to calculate it quadratic variation.
    \begin{enumerate}[label=(\arabic{*})]
    	\item By directly calculating the quadratic total variation,
    	\begin{equation*}
    		\inn{B}_t = \lim _{n \rightarrow \infty} \sum_{i=1}^{p_n}\left(B_{t_i^n}-B_{t_{i-1}^n}\right)^2=t.
    	\end{equation*}

    	\item We already know $(B_t^2 - t)_{t \geq 0}$ is a martingale. Then, by Doob-Meyer decomposition, we directly has
    	\begin{equation*}
    		\inn{B}_t = t.
    	\end{equation*}
    \end{enumerate}
    So we usually denote
    \begin{equation*}
    	dB_tdB_t = t,~dB_t dt = dt dt =0.
    \end{equation*}
\end{exam}
\begin{exam}
    If $\bm{B}=(B^1,\cdots,B^d)$ is a $d$-dim Brownian motion, then
    \begin{equation*}
        \inn{B^i,B^j}_t = \delta_{ij}t.
    \end{equation*}
    Given a partition $\Pi$ of $[0,t]$ and let
    \begin{equation*}
        S_\Pi=\sum_{k=1}^n\left(B_{t_k}^i-B_{t_{k-1}}^i\right)\left(B_{t_k}^j-B_{t_{k-1}}^j\right), \quad i \neq j.
    \end{equation*}
    Then $S_\Pi \sto \inn{B^i,B^j}_t$ by definition. But by independence
    \begin{equation*}
        \mathbb{E}\left[S_\Pi^2\right]=\sum_{k=1}^n \mathbb{E}\left[\left(\Delta_k B^i\right)^2\right] \mathbb{E}\left[\left(\Delta_k B^j\right)^2\right]=\sum_{k=1}^n\left(\Delta t_k\right)^2 \leq t\norm{\Pi} \sto 0 .
    \end{equation*}
\end{exam}

\begin{defn}[Semimartingale]
    A process $X=(X_t)_{t \geq 0}$ is called a semimartingale if 
    \begin{equation*}
    	X_t = X_0 + M_t + A_t,
    \end{equation*}
    where $M \in \mathcal{M}^{loc}_0$ and $A$ is adapted and c\`adl\`ag of bounded variation, i.e. there exists increasing adapted process $A^+,A^-$ such that
    \begin{equation*}
    	A = A^+ - A^-.
    \end{equation*}
\end{defn}
\begin{rmk}
    This decomposition may be not unique. But if $X$ is continuous, it is unique.
\end{rmk}

\begin{lem}
    A continuous local martingale with bounded variation is constant a.e..
\end{lem}
\begin{rmk}
    A continuous non-constant local martingale is of unbounded variation. Therefore, we cannot use the usual Riemannian-Stieltjes to define the stochastic integral w.s.t. martingale.
\end{rmk}

\section{Integral w.s.t. Martingale}

For $M \in \mathcal{M}^2$, the goal is to define $\int_0^T H_tdM_t$. By employing the idea from Riemannian-Stieltjes
\begin{equation*}
   \int_0^T H_tdM_t \defeq \lim_n \sum_{i=0}^n H_{\alpha_i}(M_{t_{i+1}} - M_{t_i}).
\end{equation*}
But because $M$ is not of bounded variation, it is not well-defined. So there are three choices:
\begin{enumerate}[label=(\roman*)]
    \item $\alpha_i = t_i$: It\^o integral.
    \item $\alpha_i = (t_{i+1}-t_i) /2$: Stratonovich integral.
    \item $\alpha_i = t_{i+1}$: backward It\^o integral.
\end{enumerate}
In the following, we mainly consider the It\^o integral.

\noindent \textbf{Notation:} Let $\mathcal{E}^b$ be the set of all bounded predictable simple process, i.e., if $H_t \in \mathcal{E}^b$, then
\begin{equation*}
    H_t(\omega) = \sum_{i=0}^{n-1} h^i(\omega) \mathbb{I}_{(t_i,t_{i+1}]}(t),
\end{equation*}
where $h^i$ is bounded and $\mathcal{F}_{t_i}$-measurable.

\paragraph{Integral of simple process.}

\begin{defn}
    For $H \in \mathcal{E}^b$ with $H_t(\omega) = \sum_{i=0}^{n-1} h^i(\omega) \mathbb{I}_{(t_i,t_{i+1}]}(t)$, the stochastic integral with respect to $M$ is
    \begin{equation*}
        (H \cdot M)_t = \int_0^t H_s dM_s = \sum_{i=0}^{n-1}h^i(M_{t_{i+1} \wedge t} - M_{t_i \wedge t}).
    \end{equation*}
\end{defn}

\begin{lem}
    Let $H^1,H^2 \in \mathcal{E}^b$ and $c_1,c_2 \in \R$. Then $c_1H_1 + c_2H_2 \in \mathcal{E}^b$ and
    \begin{equation*}
        (c_1H^1+ c_2H^2) \cdot M = c_1 (H^1 \cdot M) + c_2 (H^2 \cdot M).
    \end{equation*}
\end{lem}

\begin{prop}
    For $H \in \mathcal{E}^b$ and $M \in \mathcal{M}^2$, $H \cdot M \in \mathcal{M}^{2,c}_0$. Moreover,
    \begin{equation*}
        \E\bj{(H \cdot M)^2_\infty} = \E\bj{\bc{\int_0^\infty H_udM_u}^2} = \E\bj{\int_0^\infty H_u^2 d \inn{M}_u}.
    \end{equation*}
\end{prop}
\begin{proof}
    Let $s \leq t$. If $s = t_k$ and $t = t_\ell$ with $k < \ell$, then
    \begin{align*}
        \E[(H\cdot M)_t - (H \cdot M)_s \mid \mathcal{F}_s] &= \sum_{i=k}^{\ell - 1}\E\bj{h^i(M_{t_{i+1}} - M_{t_i}) \mid \mathcal{F}_{t_k}} \\
        &=\sum_{i=k}^{\ell - 1}\E\bj{\E\bj{h^i(M_{t_{i+1}} - M_{t_i}) \mid \mathcal{F}_{t_i}} \mid \mathcal{F}_{t_k}} \\
        &= \sum_{i=k}^{\ell - 1}\E\bj{h^i\E\bj{(M_{t_{i+1}} - M_{t_i}) \mid \mathcal{F}_{t_i}} \mid \mathcal{F}_{t_k}} \\
        &= 0
    \end{align*}
    It is similar for any $s \leq t$. So $H \cdot M \in \mathcal{M}^2_0$. Next,
    \begin{align*}
        \E\bj{(H \cdot M)^2_\infty} &=  \E\bj{\bc{\sum_{i=0}^{n-1} h^i(M_{t_{i+1}} - M_{t_i})}^2} \\
        &= \sum_{i=0}^{n-1}\E\bj{(h^i)^2(M_{t_{i+1}} - M_{t_i})^2} + 2\sum_{i < j}\E\bj{h_ih_j(M_{t_{i+1}} - M_{t_i})(M_{t_{j+1}} - M_{t_j})}.
    \end{align*}
    Note that
    \begin{align*}
        \E\bj{h_ih_j(M_{t_{i+1}} - M_{t_i})(M_{t_{j+1}} - M_{t_j})} &= \E \bj{\E\bj{h_ih_j(M_{t_{i+1}} - M_{t_i})(M_{t_{j+1}} - M_{t_j}) \mid \mathcal{F}_{t_j} }} \\
        &= \E \bj{h_ih_j(M_{t_{i+1}} - M_{t_i})\E\bj{(M_{t_{j+1}} - M_{t_j}) \mid \mathcal{F}_{t_j} }} \\
        &= 0.
    \end{align*}
    So
    \begin{align*}
        \E\bj{(H \cdot M)^2_\infty} &= \sum_{i=0}^{n-1}\E\bj{(h^i)^2(M_{t_{i+1}} - M_{t_i})^2} \\
        &= \sum_{i=0}^{n-1} \E\bj{(h^i)^2 \E\bj{(M_{t_{i+1}} - M_{t_i})^2 \mid \mathcal{F}_{t_i} } }.
    \end{align*}
    Note that
    \begin{align*}
        \E\bj{(M_{t_{i+1}} - M_{t_i})^2 \mid \mathcal{F}_{t_i} } &= \E\bj{M_{t_{i+1}}^2 \mid \mathcal{F}_{t_i}} + M_{t_i}^2 - 2 M_{t_i}\E\bj{M_{t_i} \mid \mathcal{F}_{t_i}}\\
        &= \E\bj{M_{t_{i+1}}^2 - M_{t_i}^2 \mid \mathcal{F}_{t_i}}.
    \end{align*}
    Because $M \in \mathcal{M}^2$, $M^2 - \inn{M}$ is a martingale, which implies that
    \begin{equation*}
        \E\bj{M_{t_{i+1}}^2 - M_{t_i}^2 \mid \mathcal{F}_{t_i}} = \E\bj{\inn{M}_{t_{i+1}} - \inn{M}_{t_i} \mid \mathcal{F}_{t_i}}.
    \end{equation*}
    Therefore,
    \begin{align*}
        \E\bj{(H \cdot M)^2_\infty} &= \sum_{i=0}^{n-1} \E\bj{(h^i)^2 \E\bj{\inn{M}_{t_{i+1}} - \inn{M}_{t_i}\mid \mathcal{F}_{t_i}}} \\
        &= \sum_{i=0}^{n-1} \E\bj{(h^i)^2 \bc{\inn{M}_{t_{i+1}} - \inn{M}_{t_i}}} \\
        &= \E\bj{\int_0^\infty H^2_u d\inn{M}_u}.
    \end{align*}
    Also because $H$ is bounded and $\E[\inn{M}] < \infty$,
    \begin{equation*}
        \E\bj{(H \cdot M)^2_t} \leq \E\bj{(H \cdot M)^2_\infty} < \infty. \qedhere
    \end{equation*}
    We omit the proof of continuity.
\end{proof}

\begin{cor}
    For $H \in \mathcal{E}^b$ and $B=(B_t)_{t \geq 0}$ a Brownian motion, 
    \begin{equation*}
        \E\bj{\int_a^b H_u dB_u} \defeq \E\bj{(H \cdot M)_b} - \E\bj{(H \cdot M)_a} = 0,
    \end{equation*}
    and
    \begin{equation*}
        \E\bj{\bc{\int_a^b H_u dB_u}^2} = \E\bj{\int_a^b H_u^2 du}.
    \end{equation*}
\end{cor}

\paragraph{Integral of $L^2$ integrable process.}

\begin{thm}
    If $M \in \mathcal{M}^{2,c}_0$ and $H$ is a progressively measurable process such that
    \begin{equation}\label{eq:integrable}
        \E\bj{\int_0^T H_s^2 d\inn{M}_s} < \infty,
    \end{equation}
    for all $T \geq 0$, then there exists a sequence of predictable simple processes $H^{(n)}$ such that
    \begin{equation*}
        \sup_{T > 0} \lim_{n \sto \infty} \E\bj{\int_0^T \abs{H_s^{(n)} - H_s}^2 d\inn{M}_s} = 0.
    \end{equation*}
\end{thm}
\begin{rmk}
    Note that because $\inn{M}$ is increasing, it can use Riemannian-Stieltjes to define
    \begin{equation*}
        \int_0^T H_s^2 d\inn{M}_s.
    \end{equation*}
\end{rmk}
\begin{rmk}
    For $M \in \mathcal{M}^{2,c}_0$ and any $H$ satisfies above condition and a corresponding sequence of predictable simple processes $H^{(n)}$, because
    \begin{equation*}
        \mathbb{E}\left[\left(\int_0^T H_s^{(n)} d M_s-\int_0^T H_s^{(m)} d M_s\right)^2\right] =\mathbb{E}\left[\int_0^T\left|H_s^{(n)}-H_s^{(m)}\right|^2 d\langle M\rangle_s\right] \longrightarrow 0,
    \end{equation*}
    due to $\int_0^TH_s^{(n)} d\inn{M}_s$ converges in $L^2$. So $\int_0^T H_s^{(n)} d M_s$ is Cauchy in $L^2$ and it is convergent in $L^2$.
\end{rmk}

\noindent \textbf{Notation:} For any $0 \leq T < \infty$, let $\mathcal{L}^*_T(M)$ be the set of all bounded progressively measurable process satisfying condition (\ref{eq:integrable}) and $\mathcal{L}^*(M) = \bigcap_{T \geq 0} \mathcal{L}_T^*$

\begin{defn}[Stochastic Integral]
    For $H \in \mathcal{L}^*_T$, the stochastic integral w.s.t. $M \in \mathcal{M}^{2,c}$ is defined by
    \begin{equation*}
        \int_0^T H_s dM_s = \lim_{n \sto \infty} \int_0^T H_s^{(n)}dM_s,
    \end{equation*}
    where and $H^{(n)}$ is a sequence satisfying (\ref{eq:integrable}) and the convergence is in $L^2$.
\end{defn}
\begin{rmk}
    \begin{enumerate}[label=(\roman*)]
        \item Note that the convergence is also in $L^1$.
        \item This definition is well-defined, i.e., independent of the choice of $H^{(n)}$. If there is another $K^{(n)}$, then we can construct $Z^{(n)}$ such that $Z^{(2n)} = H^{(n)}$ and $Z^{(2n+1)} = K^{(n)}$. So
        \begin{equation*}
            \sup_{T > 0} \lim_{n \sto \infty} \E\bj{\int_0^T \abs{Z_s^{(n)} - H_s}^2 d\inn{M}_s} = 0,
        \end{equation*}
        which implies that $\int_0^T Z_s^{(n)} d M_s$ is also Cauchy in $L^2$ and thus
        \begin{equation*}
            \lim_{n \sto \infty} \int_0^T Z_s^{(n)}dM_s = \lim_{n \sto \infty} \int_0^T H_s^{(n)}dM_s = \lim_{n \sto \infty} \int_0^T K_s^{(n)}dM_s.
        \end{equation*}
        \item If $t \mapsto \inn{M}_t$ is absolutely continuous a.e., then $\int_0^T H_s dM_s$ is well-defined if $H$ is bounded, measurable and $\mathbb{F}$-adapted.
    \end{enumerate}
\end{rmk}

\begin{prop}
    Let $M \in \mathcal{M}^{2,c}$ and $H,K \in \mathcal{L}^*_T(M)$ and $\alpha,\beta \in \R$.
    \begin{enumerate}[label=(\arabic{*})]
        \item Since $H$ is $\mathbb{F}$-adapted, $\bc{\int_0^t H_u dM_u}_{0\leq t \leq T} \in \mathcal{M}^{2,c}_0$
        \item Linearity:
        \begin{equation*}
            \int_0^T \alpha H_u + \beta K_u dM_u = \alpha\int_0^T H_u dM_u+\beta\int_0^T K_u dM_u.
        \end{equation*}
        \item Isometry:
        \begin{equation*}
            \E\bj{\abs{\int_0^T H_u dM_u}^2} = \E\bj{\int_0^T H^2_u d\inn{M}_u}.
        \end{equation*}
        \item Moreover,
        \begin{equation*}
             \E\bj{\abs{\int_s^t H_u dM_u}^2 \mid \mathcal{F}_s} = \E\bj{\int_s^t H^2_u d\inn{M}_u \mid \mathcal{F}_s}.
        \end{equation*}
        \item 
        \begin{equation*}
            \inn{\int_0^\cdot H_u dM_u}_t = \int_0^t H_u^2 d \inn{M}_u.
        \end{equation*}
    \end{enumerate}
\end{prop}
\begin{proof}
    \begin{enumerate}[label=(\arabic{*})]
        \item Choose a simple process $H^{(n)}$ to approximate $H$. Because
        \begin{equation*}
            \int_0^T H_s^{(n)}dM_s = \sum_{i=0}^{n-1}h^i(M_{T \wedge t_{i+1}} - M_{T \wedge t_i}) \in \mathcal{F}_T,
        \end{equation*}
        and
        \begin{equation*}
            \int_0^T H_sdM_s = \lim_{n \sto \infty} \int_0^T H_s^{(n)}dM_s,
        \end{equation*}
        it is $\mathcal{F}_T$-measurable. Moreover, because above convergence is in $L^2$ ($L^2$ implies $L^1$), $\int_0^T H_sdM_s \in L^1$. For any $0 \leq s < t$ and any $A \in \mathcal{F}_s$, it suffice to prove
        \begin{align*}
            \E\bj{\int_0^t H_udM_u \mathbb{I}_A} =  \E\bj{\int_0^s H_udM_u \mathbb{I}_A} ~\Leftrightarrow~ \E\bj{\int_s^t H_udM_u \mathbb{I}_A} = 0
        \end{align*}
        Note that by $L^1$-convergence,
        \begin{equation*}
            \E\bj{\int_s^t H_udM_u \mathbb{I}_A} = \lim_{n \sto \infty} \E\bj{\int_s^t H^{(n)}_udM_u \mathbb{I}_A}.
        \end{equation*}
        But because $(\int_0^t H^{(n)}_udM_u)$ is a martingale, $\E\bj{\int_s^t H^{(n)}_udM_u \mathbb{I}_A} = 0$. So
        \begin{equation*}
            \E\bj{\int_s^t H_udM_u \mathbb{I}_A} = 0.
        \end{equation*}

        \item It is directly obtained by the linearity of $\int_0^t H^{(n)}_udM_u$ and $L^1$-convergence.

        \item It is directly obtained by the same property of $\int_0^t H^{(n)}_udM_u$ and also the $L^1$-convergence.

        \item For $A \in \mathcal{F}_s$, because $ \mathbb{I}_A^2 =  \mathbb{I}_A$, by $(3)$,
        \begin{equation*}
            \E\bj{\abs{\int_s^t H_u dM_u}^2 \mathbb{I}_A} =  \E\bj{\abs{\int_s^t H_u \mathbb{I}_A dM_u}^2} =\E\bj{\int_s^t H^2_u d\inn{M}_u \mathbb{I}_A}.
        \end{equation*}

        \item For $0 \leq s < t$, by $(1)$, $(\int_0^t H_u dM_u)_{t \geq 0}$ is a martingale. So by $(4)$,
        \begin{align*}
            \E\bj{\bc{\int_0^t H_u dM_u}^2 - \bc{\int_0^s H_u dM_u}^2 \mid \mathcal{F}_s} &= \E\bj{\bc{\int_0^t H_u dM_u - \int_0^s H_u dM_u}^2 \mid \mathcal{F}_s}\\
            &= \E\bj{\int_s^t H_u d\inn{M}_u \mid \mathcal{F}_s}.
        \end{align*}
        It follows that
        \begin{equation*}
            \mathbb{E}\left[\left(\int_0^t H_u d M_u\right)^2-\int_0^t H_u^2 d\langle M\rangle_u \mid \mathcal{F}_s\right]=\left(\int_0^s H_u d M_u\right)^2-\int_0^s H_u^2 d\langle M\rangle_u.
        \end{equation*}
        Because $\int_0^t H_u^2 d\langle M\rangle_u$ is an increasing process and $\left(\int_0^t H_u d M_u\right)^2-\int_0^t H_u^2 d\langle M\rangle_u$ is a martingale, by the uniqueness of Doob-Meyer decomposition,
        \begin{equation*}
            \inn{\int_0^\cdot H_u dM_u}_t = \int_0^t H_u^2 d \inn{M}_u. \qedhere
        \end{equation*}
    \end{enumerate}
\end{proof}
\begin{rmk}
    On a probability space $(\Omega,\mathcal{F},\Pb)$, if $\mathcal{G} \subset \mathcal{F}$, then by Jensen's inequality,
    \begin{equation*}
        \abs{\E\bj{X \mid \mathcal{G}}}^p \leq \E\bj{\abs{X}^p \mid \mathcal{G}}~\Rightarrow~\norm{\E[X \mid \mathcal{G}]}_p \leq \norm{X}_p
    \end{equation*}
    for $1 \leq p < \infty$, which is also true for $p = \infty$. So for $\mathcal{G}$-measurable $X_n \sto X$ in $L^p(\mathcal{F})$, by
    \begin{equation*}
        \|\mathbb{E}[X \mid \mathcal{G}]-X\|_p \leq\left\|\mathbb{E}\left[X-X_n \mid \mathcal{G}\right]\right\|_p+\left\|X_n-X\right\|_p \leq 2\left\|X_n-X\right\|_p \sto 0,
    \end{equation*}
    $X = \E[X \mid \mathcal{G}]$ is $\mathcal{G}$-measurable. 
\end{rmk}

\begin{cor}
    Consider a Brownian motion $B$ and $H \in \mathcal{L}^*(B)$,
    \begin{align*}
        \E\bj{\int_s^t H_u dM_u \mid \mathcal{F}_s} &= 0,\\
         \E\bj{\bc{\int_s^t H_u dM_u}^2 \mid \mathcal{F}_s} &= \E\bj{\int_s^t H^2_u du \mid \mathcal{F}_s} =\int_s^t \E\bj{ H^2_u} du
    \end{align*}
\end{cor}

\begin{thm}
    Let $M,N \in \mathcal{M}^{2,c}$ and $H\in \mathcal{L}^*(M)$ and $K\in \mathcal{L}^*(N)$.
    \begin{enumerate}[label=(\arabic{*})]
        \item For stopping times $S \leq T$,
        \begin{equation*}
            \E\bj{\int_0^{t \wedge T} H_u dM_u \mid \mathcal{F}_S} = \int_0^{t \wedge S} H_u dM_u.
        \end{equation*}

        \item For a stopping time $T$,
        \begin{equation*}
            \int_0^{t \wedge T} H_u dM_u = \int_0^{t} H_u \mathbb{I}_{[0,T]} dM_u = \int_0^{t} H_u dM_{u \wedge T}.
        \end{equation*}

        \item For stopping times $S \leq T$,
        \begin{equation*}
            \E\bj{\bc{\int_{t \wedge S}^{t \wedge T} H_u dM_u}\bc{\int_{t \wedge S}^{t \wedge T} K_u dN_u} \mid \mathcal{F}_S} = \E\bj{\bc{\int_{t \wedge S}^{t \wedge T} H_uK_u d\inn{M,N}_u} \mid \mathcal{F}_S}.
        \end{equation*}
        In particular, if $S,T$ are constant
        \begin{equation*}
            \E\bj{\bc{\int_s^t H_u dM_u}\bc{\int_s^t K_u dN_u} \mid \mathcal{F}_s} = \E\bj{\bc{\int_s^t H_uK_u d\inn{M,N}_u} \mid \mathcal{F}_s}.
        \end{equation*}
        Moreover, it follows that
        \begin{equation*}
            \inn{\int_0^\cdot H_udM_u,\int_0^\cdot K_u dN_u}_t = \E\bj{\int_0^t H_uK_u d\inn{M,N}_u}.
        \end{equation*}
        In particular,
        \begin{equation*}
            \inn{\int_0^\cdot H_udM_u,N}_t = \E\bj{\int_0^t H_u d\inn{M,N}_u}.
        \end{equation*}

        \item If $G \in \mathcal{L}^*\bc{ \int_0^\cdot H_u dM_u }$, then $GH \in \mathcal{L}^*(M)$ and
        \begin{equation*}
            \int_0^t G_s d\bc{\int_0^s H_u dM_u} = \int_0^tG_uH_u dM_u.
        \end{equation*}
    \end{enumerate}
\end{thm}

\begin{prop}[Kunita-Watanabe]
    Let $M,N \in \mathcal{M}^{2,c}_0$ and $H\in \mathcal{L}^*(M)$ and $K\in \mathcal{L}^*(N)$. Then
    \begin{equation*}
        \int_0^t \abs{H_uK_u} d \inn{M,N}_u \leq \bc{\int_0^t H_u^2 d\inn{M}_u}^\frac{1}{2}\bc{\int_0^t K_u^2 d\inn{N}_u}^\frac{1}{2}.
    \end{equation*}
\end{prop}

\begin{rmk}
    Condition (\ref{eq:integrable}) can be weaker as
    \begin{equation*}
        \Pb\bc{\int_0^T H^2_u d\inn{M}_u < \infty} = 1,
    \end{equation*}
    but the convergence
    \begin{equation*}
        \int_0^T H_u dM_u = \lim_{n \sto \infty} \int_0^T H^{(n)}_u dM_u
    \end{equation*}
    is weaker to in probability. In such case, $\bc{\int_0^t H_u dM_u}_{t \geq 0}$ is not a martingale, but a local martingale.
\end{rmk}

\section{Integral w.s.t. Local (Semi) Martingale}

\paragraph{Local martingale.}

\begin{defn}
    For $M \in \mathcal{M}^{c,loc}$ and $X \in \mathcal{L}^*(M)$, i.e.
    \begin{equation*}
        \E\bj{\int_0^T X^2_u d\inn{M}_u} < \infty,\quad\forall~T,
    \end{equation*}
    the stochastic integral of $X$ w.s.t. $M$ is defined by
    \begin{equation*}
        \int_0^t X_s dM_s \defeq \int_0^t X_s \mathbb{I}_{\bb{T_n \geq s}}dM_{s \wedge T_n}
    \end{equation*}
    on $\bb{0 \leq t \leq T_n}$, where $T_n \uparrow \infty$ is the sequence such that $(M_{t \wedge T_n})_{t \geq 0}$ is a UI martingale.
\end{defn}
\begin{rmk}
    Here we do not need the condition of $L^2$-integrability for martingale because such $T_n$ can be chosen such that $(M_{t \wedge T_n})_{t \geq 0}$ is $L^2$-integrable.
\end{rmk}

\begin{thm}
    Let $M \in \mathcal{M}^{c,loc}$ and $X,Y \in \mathcal{L}^*(M)$.
    \begin{enumerate}[label=(\arabic{*})]
        \item $\bc{\int_0^t X_udM_u}$ is a continuous local martingale, i.e., in  $\mathcal{M}^{c,loc}_0$.
        \item Linearity:
        \begin{equation*}
            \int_0^t\left(\alpha X_s+\beta Y_s\right) d M_s=\alpha \int_0^t X_s d M_s+\beta \int_0^t Y_s d M_s.
        \end{equation*}
        \item Quadratic variation:
        \begin{equation*}
            \left\langle\int_0 X_s d M_s\right\rangle_t=\int_0^t X_s^2 d\langle M\rangle_s.
        \end{equation*}
        \item For stopping time $T$,
        \begin{equation*}
            \int_0^{t \wedge T} X_s d M_s=\int_0^t X_s I_{\{s \leq T\}} d M_s.
        \end{equation*}
    \end{enumerate}
\end{thm}
\begin{rmk}
    Note that for the properties related to expectation cannot be extended to local martingale, like
    \begin{equation*}
        \mathbb{E}\left[\left(\int_0^t X_u d M_u\right)^2\right]\neq \mathbb{E}\left[\int_0^t X_u^2 d\langle M\rangle_u\right],\quad \mathbb{E}\left[\left(\int_s^t X_u d M_u\right)^2 \mid \mathcal{F}_s\right] \neq \mathbb{E}\left[\int_s^t X_u^2 d\langle M\rangle_u \mid \mathcal{F}_s\right],
    \end{equation*}
    in general.
\end{rmk}

\paragraph{Semimartingale.} Recall $X = (X_t)_{t \geq 0}$ is a semimartingale if
\begin{equation*}
    X_t = X_0 + M_t^X + A_t^X,
\end{equation*}
where $M^X = (M_t^X)_{t \geq 0}$ is a local martingale with $M_0^X = 0$ and $A^X = (A_t^X)_{t \geq 0}$ is a c\`adl\`ag, adapted process of bounded variation. Note that this decomposition if not unique unless $X$ is continuous. In the following, we consider continuous $X$.

\begin{defn}
    Let $X$ be a continuous semimartingale. For $H \in \mathcal{L}^*(M^X)$, define
    \begin{equation*}
        \int_0^t H_s dX_s = \int_0^t H_s dM^X_s + \int_0^tH_s dA^X_s,
    \end{equation*}
    where the second integral is the Riemannian-Stieltjes integral.
\end{defn}

\noindent Next, we need to define the quadratic variation for general case.
\begin{defn}
    Let $X,Y$ be semimartingales.
    \begin{enumerate}[label=(\arabic{*})]
        \item The quadratic variation of $X$ is defined as
        \begin{equation*}
            [X,X]_t \defeq \lim_{n \sto \infty} \sum_{t_i \in \Pi_n,t_i \leq t} \bc{X_{t_{i+1} \wedge t} - X_{t_i \wedge t}}^2.
        \end{equation*}

        \item The cross variation of $X,Y$ is
        \begin{equation*}
            [X,Y]_t = \frac{1}{4}\bc{[X+Y,X+Y]_t - [X-Y,X-Y]_t}.
        \end{equation*}
    \end{enumerate}
\end{defn}
\begin{rmk}
    If $X,Y$ are two continuous local martingale, then $[X,X]_t = \inn{X}_t$ and $[X,Y]_t = \inn{X,Y}_t$.
\end{rmk}

\begin{thm}
    If $X,Y$ are semimartingales and let $M^{X,c},M^{Y,c}$ be their continuous local martingale parts, then
    \begin{equation*}
        [X,Y]_t = \inn{M^{X,c},M^{Y,c}}_t + \sum_{s\leq t} \Delta X_s\Delta Y_s,
    \end{equation*}
    where $\Delta X_s = X_s - X_{s-}$.
\end{thm}
\begin{rmk}
    In particular, if $X,Y$ are continuous semimartingales, then
    \begin{equation*}
        [X,Y]_t = \inn{M^X,M^Y}_t.
    \end{equation*}
\end{rmk}

\begin{cor}
    If $X,Y$ are continuous semimartingale and $Y \in \mathcal{L}^*(M^X)$, then
    \begin{equation*}
        \bj{\int_0^\cdot H_s dX_s,Y}_t = \int_0^t H_s d[X,Y]_s.
    \end{equation*}
\end{cor}

\begin{thm}[DCT]
    Let $X$ be a continuous semimartingale with the decomposition $X_t = X_0 + M_t+ A_t$. Let $H^{(n)}$ and $H$ be locally bounded progressive processes, and let $K$ be a nonnegative progressive process. If
    \begin{enumerate}[label=(\roman*)]
        \item $H_s^{(n)} \sto H_s$ a.e. for any $s \in [0,t]$,
        \item $\abs{H^{(n)}_s} \leq K_s$ a.e. for any $n$ and $s \in [0,t]$,
        \item $K_s \in \mathcal{L}^*$ and $\int_0^t\abs{K_s}\abs{dA_s} < \infty$,
    \end{enumerate}
    then
    \begin{equation*}
        \int_0^t H_s^{(n)} dX_s \sto \int_0^t H_s dX_s.
    \end{equation*}
\end{thm}

\section{It\^o Formula}

\begin{thm}[$1$-dim, Continuous Form]
    Let $f \colon \R \sto \R$ be a $C^2$-function and $X=(X_t)_{t \geq 0}$ be a continuous semimartingale with the decomposition $X_t = X_0 + M_t + A_t$. Then
    \begin{align*}
        f(X_t) &= f(X_0) + \int_0^t f^\prime(X_0)dX_u + \frac{1}{2} \int_0^t f^{\prime\prime}(X_u)d[X,X]_u\\
        &= f(X_0) + \int_0^t f^\prime(X_0) dM_u + \int_0^t f^\prime(X_0) dA_u + \frac{1}{2}\int_0^t f^{\prime\prime}(X_u)d\inn{M}_u.
    \end{align*}
\end{thm}
\begin{proof}[Sketch of Proof]
    By Taylor formula,
    \begin{equation*}
        f(X_{t_{i+1} \wedge t}) -f(X_{t_i}) = f^\prime(X_{t_i}) \Delta_i X + \frac{1}{2} f^{\prime \prime}(X_{t_i})(\Delta_i X)^2 + R_i.
    \end{equation*}
    By taking the summation,
    \begin{equation*}
        f(X_t) - f(X_0) = \sum_i f^\prime(X_{t_i}) \Delta_i X + \frac{1}{2} \sum_i f^{\prime \prime}(X_{t_i})(\Delta_i X)^2 + \sum_i R_i.
    \end{equation*}
    By the definition of integral,
    \begin{equation*}
        \sum_i f^\prime(X_{t_i}) \Delta_i X  \sto \int_0^t f^\prime(X_u)dX_u.
    \end{equation*}
    By the definition  of quadratic variation,
    \begin{equation*}
         \sum_i f^{\prime \prime}(X_{t_i})(\Delta_i X)^2 \sto \int_0^t f^{\prime \prime}(X_u)d[X,X]_u.
    \end{equation*}
    For $R_i$, because $f \in C^2$
    \begin{equation*}
        \abs{\sum_i R_i} \leq \frac{1}{2}\sum_i \abs{f^{\prime \prime}(\xi_i) - f^{\prime \prime}(X_{t_i})}\abs{\Delta_iX} \leq \frac{1}{2}\sum_i \abs{\Delta_iX}^2 \sto 0. \qedhere
    \end{equation*}
\end{proof}
\begin{rmk}
    \begin{enumerate}[label=(\roman*)]
        \item Note that $\int_0^t f^\prime(X_0) dM_u$ is a continuous local martingale, and $\int_0^t f^\prime(X_0) dA_u + \int_0^t f^{\prime\prime}(X_u)d\inn{M}_u$ is of bounded variation (because integral of bounded variation is still of bounded variation). So $f(X_t)$ is also a continuous semi-martingale. 
        \item It has the differential form
        \begin{align*}
            df(X_t) &= f^\prime(X_t)dX_t + \frac{1}{2}f^{\prime\prime}(X_t)d[X,X]_t\\
            &= f^\prime(X_t) dM_t + f^\prime(X_t)dA_t +\frac{1}{2}f^{\prime\prime}(X_t)d\inn{X}_t,
        \end{align*}
        which is just a formula.
        \item If $X$ is a continuous semimartingale, we denote $(dX_t)^2 = d [X,X]_t$. Therefore, the different form becomes
        \begin{equation*}
            df(X_t) = f^\prime(X_t)dX_t + \frac{1}{2}f^{\prime\prime}(X_t)(dX_t)^2.
        \end{equation*}
        In particular, for $X=B$ a Brownian motion, $(dB_t)^2 = dt$. For example, if $dX_t = fdB_t + g dt$, then
        \begin{equation*}
            (dX_t)^2 = f^2(dB_t)^2 + 2fg dB_t dt + g^2(dt)^2 = f^2 (dB_t)^2 = f^2 dt.
        \end{equation*}
    \end{enumerate}
\end{rmk}

\begin{exam}
    \begin{enumerate}[label=(\arabic*)]
        \item Let $f(x) = x^2$ and $X=B$ a Brownian motion. Then
        \begin{equation*}
            B_t^2 = B_0^2 + 2\int_0^t B_s dB_s + \int_0^tds~\Rightarrow~ \int_0^t B_s dB_s = \frac{1}{2}\bc{B_t^2 - t}.
        \end{equation*}

        \item Let $W$ be a Brownian motion and $X \in \mathcal{L}^*(W)$. Consider the process
        \begin{equation*}
            Z_t = \exp\bc{\int_0^t X_u dW_u - \frac{1}{2}\int_0^tX_u^2 du}.
        \end{equation*}
        Let
        \begin{equation*}
            Y_t = \int_0^t X_u dW_u - \frac{1}{2}\int_0^tX_u^2 du,
        \end{equation*}
        which is a semimartingale, or informally, $dY_t = X_t dW_t - \frac{1}{2}X_t^2 dt$. Let $f(x) = e^x$. Then
        \begin{align*}
            dZ_t &= f^\prime(Y_t)dY_t + \frac{1}{2}f^{\prime\prime}(Y_t)(dY_t)^2 \\
            &= Z_t \bc{X_t dW_t - \frac{1}{2}X_t^2 dt} + \frac{1}{2}Z_t X_t^2 dt \\
            &= Z_tX_t dW_t.
        \end{align*}
        Therefore, 
        \begin{equation*}
            Z_t = Z_0 + \int_0^t Z_uX_u dW_u = 1+ \int_0^t Z_uX_u dW_u
        \end{equation*}
        Moreover, $Z_t = \exp\bc{\int_0^t X_u dW_u - \frac{1}{2}\int_0^tX_u^2 du}$ is a solution of SDE
        \begin{equation*}
            dZ_t = Z_tX_t dW_t.
        \end{equation*}
        In particular, if $X_t \equiv \sigma$, then
        \begin{equation*}
            Z_t = \exp\bc{\sigma W_t - \frac{1}{2}\sigma^2 t}
        \end{equation*}
        is a solution of $dZ_t = \sigma Z_tdW_t$.
        \begin{rmk}
            Note that $Z$ is a continuous local martingale. In fact, for $X$ with $\Pb(\int_0^TX_u^2du < \infty) = 1$, it is also a local martingale.
        \end{rmk}
    \end{enumerate}
\end{exam}

\begin{thm}[Multi-dim, Local Martingale, Continuous Form]
    Let $\bm{X} = (X^1,\cdots,X^n)$ be a vector of continuous local martingales. Let $f \colon [0,\infty) \times \R^{n} \sto \R$ be $C^{1,2}$.
    \begin{align*}
        f(\bm{X}_t) &= f(0,\bm{X}_0) + \int_0^t \frac{\partial}{\partial t} f(s,\bm{X}_s)dt + \sum_{i=1}^n \int_0^t \frac{\partial}{\partial x_i} f(s,\bm{X}_s)dX_s^i \\ 
        &\quad + \frac{1}{2}\sum_{i,j=1}^n\int_0^t \frac{\partial^2}{\partial x_i \partial x_j}f(s,\bm{X}_s)d\inn{X^i,X^j}_s.
    \end{align*}
    or in differential form
    \begin{equation*}
        df(X_t) = \frac{\partial}{\partial t} f(t,\bm{X}_t)dt + \sum_{i=1}^n\frac{\partial}{\partial x_i} f(t,\bm{X}_t)dX_t^i+\sum_{i,j=1}^n\frac{\partial^2}{\partial x_i \partial x_j}f(t,\bm{X}_t)d\inn{X^i,X^j}_t.
    \end{equation*}
\end{thm}

\begin{exam}
    Let $\bm{W} = (W^1,\cdots,W^n)$ be a $n$-dimensional standard Brownian motion with $n \geq 2$. Let
    \begin{equation*}
        R_t = \norm{\bm{W}} = \sqrt{\sum_{i=1}^n (W^1)^2 + \cdots + (W^n)^2},
    \end{equation*}
    called the Bessel process. Let $f(\bm{x}) = \norm{x}$. Then $\frac{\partial}{\partial x_i}f(\bm{x}) = x_i / f(\bm{x})$ and 
    \begin{equation*}
        \frac{\partial^2}{\partial x_i \partial x_j}f(\bm{x}) = \begin{cases}
            -\frac{x_ix_j}{f(\bm{x})^3},& i \neq j \\
            \frac{f(\bm{x})^2 - x_i^2}{f(\bm{x})^3},&i=j.
        \end{cases}
    \end{equation*}
    Then we have
    \begin{align*}
        dR_t & = \sum_{i=1}^n \frac{\partial}{\partial x_i}f(\bm{W}_t) dW^i_t = \frac{1}{2} \sum_{i,j = 1}^n \frac{\partial^2}{\partial x_i \partial x_j}f(\bm{W}_t)d \inn{B^i,B^j}_t \\
        &= \sum_{i=1}^n \frac{W^i_t}{R_t} dW^i_t + \frac{1}{2} \sum_{i=1}^n \frac{R_t^2 - (W^t_i)^2}{R_t^3}dt \\
        &= \sum_{i=1}^n \frac{W^i_t}{R_t} dW^i_t + \frac{1}{2} \frac{n- 1}{R_t}dt.
    \end{align*}
    Therefore,
    \begin{equation*}
        R_t dR_t = \sum_{i=1}^n W^i_t dW^i_t + \frac{n-1}{2}dt.
    \end{equation*}
\end{exam}

\begin{thm}[It\^o Formula]
    Let $\bm{X} = (X^1,\cdots,X^n)$ be a $n$-dim semimartingale with decomposition
    \begin{equation*}
        X_t^i = X_0^i + M_t^i + A_t^i,\quad i = 1,\cdots,n
    \end{equation*}
    (Note that because the decomposition is not unique, it should be given.) Let $f \colon \R^n \sto \R$ be $C^2$. Then $f(\bm{X})$ is a semimartingale and
    \begin{align*}
        f(\bm{X}_t) &= f(\bm{X}_0) + \sum_{i=1}^n \int_0^t \frac{\partial}{\partial x_i} f(\bm{X}_s)dX_s^i +  \frac{1}{2}\sum_{i,j=1}^n\int_0^t \frac{\partial^2}{\partial x_i \partial x_j}f(\bm{X}_s)d\inn{M^{i,c},M^{j,c}}_s \\
        &\quad + \sum_{s \leq t}\bc{f(\bm{X}_s) - f(\bm{X}_{s-}) - \sum_{i=1}^n \frac{\partial}{\partial x_i}f(\bm{X}_{s-}) \Delta X^i_{s-} }.
    \end{align*}
\end{thm}

\paragraph{Application: Integration by parts.} 

\begin{exam}
    Given a standard Brownian motion $W$. Consider $\int_0^t s dW_s$. Let $f(t,x) = tx$. Then
    \begin{equation*}
        \frac{\partial}{\partial t}f(t,x) = x,~\frac{\partial}{\partial x}f(t,x) =t,~\frac{\partial^2}{\partial t \partial x}f(t,x) = 0.
    \end{equation*}
    So
    \begin{equation*}
        tW_t = \int_0^tW_s ds + \int_0^t s dW_s~\Rightarrow~\int_0^t s dW_s = tW_t - \int_0^tW_s ds.
    \end{equation*}
\end{exam}

\begin{thm}
    Let $f(s,\omega)$ is continuous of bounded variation w.s.t. for a.e. $\omega$. Then
    \begin{equation*}
        \int_0^t f(s)dW_s = f(t)W_t - \int_0^t W_s df(s).
    \end{equation*}
\end{thm}

\begin{thm}[Integration by parts]
    Suppose $X,Y$ are continuous semimartingale, then
    \begin{equation*}
        \int_0^tX_s dY_s = X_tY_t - X_0Y_0 + \int_0^tY_sdX_s - [X,Y]_t.
    \end{equation*}
    on informally,
    \begin{equation*}
        d(X_tY_t) = X_tdY_t + Y_tdX_t + d[X,Y]_t.
    \end{equation*}
\end{thm}
\begin{proof}
    It can be obtained by It\^o formula on $f(x,y) = xy$.
\end{proof}
\begin{rmk}
    In general, it can be written as
    \begin{equation*}
        d(X_tY_t) = X_tdY_t + Y_tdX_t + (dX_t)(dY_t),
    \end{equation*}
    where $X_t,Y_t$ can be continuous semimartingales ($M_t = 0$, i.e., as above theorem, or $M_t \neq 0$) or deterministic and for calculating $(dX_t)(dY_t)$, we can use
    \begin{equation*}
        (dB_t)^2 = 0,~dtdB_t = (dt)^2 = 0.
    \end{equation*}
\end{rmk}
\begin{rmk}
    If $X,Y$ are semimartingale,
    \begin{equation*}
        X_tY_t = X_0Y_0 + \int_0^tX_{u-}dY_u +  \int_0^t Y_{u-}dX_u + [X,Y]_t.
    \end{equation*}
    In particular,
    \begin{equation*}
        X_t^2 = X_0^2 + 2\int_0^t X_{u-}dX_u + [X,X]_t.
    \end{equation*}
\end{rmk}

\section{Martingale Representation Theorem}

The problem is whether a martingale can be represented as
\begin{equation*}
    M_t = M_0 + \int_0^tH_s dB_s.
\end{equation*}


\begin{exam}
    Let $W^1,W^2$ be two independent Brownian motions and $\mathcal{F}_t =\sigma(W^1_s,W^2_s \colon s\leq t)$. Then $W^1,W^2$ are two martingales w.s.t. $\mathbb{F} = (\mathcal{F}_t)$. If
    \begin{equation*}
        W_t^2 = \int_0^t H_s dW^1_s
    \end{equation*}
    for some $H \in \mathcal{L}^*(W^1)$, then
    \begin{equation*}
        t = \inn{W^2,W^2}_t = \inn{\int_0^\cdot H_sdW^1,W^2}_t = \int_0^t H_s d\inn{W^1,W^2}_s = 0,
    \end{equation*}
    which induces a contradiction.
\end{exam}

\begin{thm}[Martingale Representation Theorem]
    Let $\bm{B}$ be a $n$-dimensional Brownian motion w.s.t. its natural filtration $\mathbb{F}^B$. Let $M$ be a martingale w.s.t. $\mathbb{F}$ and $\mathbb{F}^B$ that is in $\mathcal{M}^2$ and c\`adl\`ag. Then there exist $H^i \in \mathcal{L}^*$ for all $i$ such that
    \begin{equation*}
        M_t = M_0 + \sum_{i=1}^n\int_0^t H_s^i dB^i_s.
    \end{equation*}
\end{thm}
\begin{rmk}
   Note that $\mathcal{F}^B_t = \sigma(B^1_s,\cdots,B^n_s \colon s \leq t)$. 
\end{rmk}

\section{Girsanov Theorem}
Fix $(\Omega,\mathcal{F},\Pb)$ with filtration $\mathbb{F} = (\mathcal{F}_t)_{t \geq 0}$ satisfying the usual condition. Let $W=(W_t)_{t \geq 0}$ be a Brownian motion on $(\Omega,\mathcal{F},\Pb)$. Denote $\E = \E_{\Pb}$.

\begin{exam}
    Let $Z_1,\cdots,Z_n \stackrel{i.i.d.}{\sim} \mathcal{N}(0,1)$ on $(\Omega,\mathcal{F},\Pb)$. Let
    \begin{equation*}
        d\tilde{\Pb}(\omega) = \exp\bc{\sum_{i=1}^n \mu_i Z_i - \frac{1}{2}\sum_{i=1}^n \mu_i^2} d\Pb(\omega).
    \end{equation*}
    Denote $\tilde{\E} = \E_{\tilde{\Pb}}$. Consider the characteristic equation,
    \begin{align*}
        \tilde{\E}\bj{\exp(it_1Z_1 + \cdots it_nZ_n)} & = \int_\Omega \exp(it_1Z_1 + \cdots it_nZ_n) d\tilde{\Pb} \\
        &= \int_\Omega \exp(it_1Z_1 + \cdots it_nZ_n) \frac{d\tilde{\Pb}}{d\Pb} d\Pb \\
        &= \int_\Omega \exp\bc{\sum_{j=1}^n \bc((it_j+\mu_j)Z_j -\frac{1}{2}\mu_j^2)} d\Pb \\
        &= \E\bj{\prod_{j=1}^n \exp\bc{(it_j+\mu_j)Z_j - \frac{1}{2}\mu_j^2}} \\
        &= \prod_{j=1}^n\E\bj{ \exp\bc{(it_j+\mu_j)Z_j - \frac{1}{2}\mu_j^2}} \\
        &= \prod_{j=1}^n \exp \left(-\frac{t_j^2}{2}+i t_j \mu_j\right).
    \end{align*}
    It follows that $Z_1,\cdots,Z_n \stackrel{i.i.d.}{\sim} \mathcal{N}(\mu_i,1)$ on $\tilde{\Pb}$.
\end{exam}

\noindent Let $X$ be a measurable, $\mathbb{F}$-adapted stochastic process such that
\begin{equation*}
    \Pb\bc{\int_0^T X_u^2 du < \infty} = 1,\quad \forall~T \geq 0.
\end{equation*}
Define
\begin{equation*}
    Z_t \defeq \exp\bc{\int_0^tX_udW_u - \frac{1}{2}\int_0^tX_u^2du}.
\end{equation*}
Then by above
\begin{equation*}
    Z_t = 1 + \int_0^tZ_uX_u dW_u,
\end{equation*}
$Z =(Z_t)_{t \geq 0}$ is a continuous local martingale.

\begin{prop}[Novikov Condition]
    If $X$ be a measurable, $\mathbb{F}$-adapted stochastic process such that
    \begin{equation*}
        \E\bj{\exp\bc{\frac{1}{2} \int_0^T X_u^2 du}} < \infty,\quad \forall~ 0\leq T < \infty,
    \end{equation*}
    then $Z$ is a martingale.
\end{prop}
\begin{rmk}
    Note that by Jensen's inequality, if $X$ satisfies Novikov condition, it is in $\mathcal{L}^*(W)$.
\end{rmk}

\noindent Define $\tilde{\Pb}_t$ on $\mathcal{F}_t$ by,
\begin{equation*}
    \tilde{\Pb}_t(A) \defeq \int_A Z_t d\Pb,\quad \forall~ A \in \mathcal{F}_t,~\Rightarrow~ Z_t = \frac{d\tilde{\Pb}_t}{d\Pb}.
\end{equation*}
\begin{rmk}
    If $Z = (Z_t)_{t \geq 0}$ is a martingale, then
    \begin{enumerate}[label=(\roman*)]
        \item $\tilde{\Pb}_t$ is a probability measure because $\E[Z_t] = \E[Z_0] = 1$.
        \item for any $s \leq t$ and any $A \in \mathcal{F}_s$,
        \begin{equation*}
            \tilde{\Pb}_s(A) = \tilde{\Pb}_t(A)
        \end{equation*}
        because of the martingale property of $Z$.
    \end{enumerate}
\end{rmk}

\begin{thm}[Girsanov Theorem]
    Assume that $Z=(Z_t)_{t \geq 0}$ defined as above is a martingale. Define a process $\tilde{W}$ as
    \begin{equation*}
        \tilde{W}_t = W_t - \int_0^t X_u du.
    \end{equation*}
    Then for each fixed $T \in [0,\infty)$, $(\tilde{W}_t)_{t \in [0,T]}$ is a Brownian motion on $(\Omega,\mathcal{F}_T, \tilde{P}_T)$.
\end{thm}

\begin{cor}
    Under the same assumption of above theorem, suppose $f$ is a measurable function such that $f(W_t) \in L^1$. Then
    \begin{equation*}
        \E_{\Q}[f(\tilde{W}_t)] = \E[f(W_t)].
    \end{equation*}
\end{cor}

\begin{exam}
    \begin{enumerate}[label=(\arabic{*})]
        \item Suppose $X_t = h(t)$, a deterministic function, that is in $L^2([0,T])$. Since
        \begin{equation*}
            \E\bj{\frac{1}{2}\exp \bc{\int_0^T h^2(u)du}} = \frac{1}{2}\exp \bc{\int_0^T h^2(u)du} < \infty,
        \end{equation*}
        by above theorem
        \begin{equation*}
            Z_t = \exp \bc{\int_0^th(u)dW_u - \frac{1}{2}\int_0^th^2(u)du}
        \end{equation*}
        is a martingale. So
        \begin{equation*}
            \tilde{W}_t = W_t - \int_0^t h(u)du
        \end{equation*}
        is a Brownian motion w.s.t. $\tilde{\Pb}$ defined as,
        \begin{equation*}
            d\tilde{\Pb} = \exp\bc{\int_0^Th(u)dW_u - \frac{1}{2}\int_0^Th^2(u)du} d\Pb.
        \end{equation*}

        \item Consider $X_t = \sgn (W_t)$ (i.e. $X_t = 1$ for $W_t \geq 0$ and $X_t = -1$ for $W_t < 0$).
        \begin{equation*}
            \E\bj{\exp\bc{\frac{1}{2}} \int_0^T X_u^2 du} = \exp\bc{\frac{T}{2}} < \infty.
        \end{equation*}
        So
        \begin{equation*}
            \tilde{W}_t = W_t - \int_0^t \sgn(W_u)du
        \end{equation*}
        is a Brownian motion w.s.t. $\Q$ given by
        \begin{equation*}
            d\Q = \exp \bc{\int_0^T \sgn(W_u)du -\frac{T}{2}}d\Pb.
        \end{equation*}
    \end{enumerate}
\end{exam}

\begin{rmk}
    Suppose $Y=(Y_t)_{t \geq 0}$ is a martingale w.s.t. to $d\Q = Z_T d\Pb$, i.e.,
    \begin{equation*}
        Y_s = \E_\Q\bj{Y_t \mid \mathcal{F}_s} = \frac{\E[Y_tZ_T \mid \mathcal{F}_s]}{\E[Z_T \mid \mathcal{F}_s]} = \frac{\E[Y_tZ_T \mid \mathcal{F}_s]}{Z_s}.
    \end{equation*}
    In particular,
    \begin{equation*}
        \E\bj{Y_TZ_T \mid \mathcal{F}_s} = Y_sZ_s.
    \end{equation*}
\end{rmk}

\section{Local Times}

Let $W=(W_t)$ be a standard Brownian motion. Define the level set
\begin{equation*}
    \bb{0 \leq t < \infty \colon W_t(\omega) = x}.
\end{equation*}
Obviously, its Lebesgue measure is $0$.

\begin{defn}
    For any Borel set $B \in \mathcal{B}$, define the occupation time of $B$ as
    \begin{equation*}
        \Gamma_t(B) = \int_0^t \mathbb{I}_{W_s \in B} ds = m\bc{\bb{ s \in [0,t] \colon W_s \in B }}.
    \end{equation*}
    Note that it is a random variable.
\end{defn}
\begin{rmk}
    The stochastic process $\Gamma(B) = (\Gamma_t(B))_{t\geq 0}$ is adapted and continuous.
\end{rmk}

\begin{defn}[Local Time]
    For a given Brownian motion $W$, the local time is defined as
    \begin{equation*}
        L_t(x) = L_t(x,\omega) = \lim_{\varepsilon \downarrow 0} \frac{1}{2\varepsilon}\Gamma_t([x-\varepsilon,x+\varepsilon]).
    \end{equation*}
    Note it is a random variable.
\end{defn}
\begin{rmk}
    Local time serves as a density function  w.s.t. the Lebesgue measure for the occupation time, i.e.,
    \begin{equation*}
        \Gamma_t(B,\omega) = \int_B L_t(x,\omega)dx.
    \end{equation*}
\end{rmk}

\begin{thm}
    The local times of a Brownian motion exist.
\end{thm}

\noindent Consider $g(x) = \abs{x}$. For $\varepsilon > 0$, let
\begin{equation*}
    g_\varepsilon(x) = \begin{cases}
        \abs{x},& \abs{x} > \varepsilon \\
        \frac{1}{2}\bc{\varepsilon+ \frac{x^2}{\varepsilon}},& \abs{x} \leq \varepsilon.
    \end{cases}
\end{equation*}
Then $g_\varepsilon \in C^1$ and
\begin{equation*}
    g^\prime_\varepsilon(x) = \begin{cases}
        1,&x \geq \varepsilon \\
        \frac{x}{\varepsilon},&\abs{x} < \varepsilon \\
        -1,& x < -\varepsilon.
    \end{cases}
\end{equation*}
Moreover, for $\abs{x} \neq \varepsilon$,
\begin{equation*}
    g^{\prime\prime}_\varepsilon(x) = \begin{cases}
        0,&\abs{x} > \varepsilon \\
        \frac{1}{\varepsilon},&\abs{x} < \varepsilon.
    \end{cases}
\end{equation*}
Therefore,
\begin{align*}
    g_\varepsilon(W_t) &= g_\varepsilon(W_0) + \int_0^t g_\varepsilon^\prime(W_s)dW_s + \frac{1}{2}\int_0^t g^{\prime\prime}(W_s)ds \\
    &= g_\varepsilon(0) + \int_0^t g_\varepsilon^\prime(W_s)dW_s + \frac{1}{2} \int_0^t \frac{1}{\varepsilon}\mathbb{I}_{\bb{\abs{W_s} < \varepsilon} } ds \\
    &= \frac{\varepsilon}{2} + \int_0^t g_\varepsilon^\prime(W_s)dW_s + \frac{1}{2 \varepsilon} \Gamma_t([-\varepsilon],-\varepsilon).
\end{align*}
For the second term,
\begin{align*}
    \int_0^t g_\varepsilon^\prime(W_s)dW_s & = \int_0^t g_\varepsilon^\prime(W_s)\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}dW_s +\int_0^t g_\varepsilon^\prime(W_s)\mathbb{I}_{\bb{\abs{W_s} > \varepsilon}}dW_s \\
    &= \int_0^t g_\varepsilon^\prime(W_s)\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}dW_s +\int_0^t \sgn(W_s)\mathbb{I}_{\bb{\abs{W_s} > \varepsilon}}dW_s. \\
\end{align*}
Note that as $\varepsilon \sto 0$, by the DCT for stochastic integral,
\begin{equation*}
    \int_0^t \sgn(W_s)\mathbb{I}_{\bb{\abs{W_s} > \varepsilon}}dW_s \sto \int_0^t\sgn(W_s)dW_s.
\end{equation*}
For the other one,
\begin{equation*}
    I = \int_0^t g_\varepsilon^\prime(W_s)\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}dW_s = \int_0^t \frac{W_s}{\varepsilon}\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}dW_s 
\end{equation*}
Note that because $I$ is a martingale, $\E[I] = 0$ and 
\begin{align*}
    \E[I^2] &= \E\bj{\int_0^t\frac{W^2_s}{\varepsilon^2}\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}ds} \\
    &=\int_0^t\frac{1}{\varepsilon^2}\E\bj{W^2_s\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}}ds \\
    &\leq \int_0^t\E\bj{\mathbb{I}_{\bb{\abs{W_s} \leq \varepsilon}}}ds \\
    &= \int_0^t \Pb(\abs{W_s} \leq \varepsilon)ds \\
    &= \int_0^t \Pb(\abs{W_1} \leq \varepsilon / \sqrt{s})ds \\
    &=  \int_0^t \frac{1}{\sqrt{2 \pi}}\int_{-\varepsilon / \sqrt{s}}^{\varepsilon / \sqrt{s}} e^{-y^2}dyds \sto 0.
\end{align*}
Therefore, as $\varepsilon \sto 0$, we get the Tanaka formula,
\begin{equation*}
    \abs{W_t} = \int_0^t \sgn(W_s)dW_s + L_t.
\end{equation*}
\begin{cor}
    Fix $a \in \R$,
    \begin{align*}
        \left|W_t-a\right| & =|a|+\int_0^t \operatorname{sign}\left(W_s-a\right) d W_s+L_t(a) \\
        \left(W_t-a\right)^{+} & =(-a)^{+}+\int_0^t \mathbb{I}_{(a, \infty)}\left(W_s\right) d W_s+\frac{1}{2} L_t(a) \\
        \left(W_t-a\right)^{-} & =(-a)^{-}-\int_0^t \mathbb{I}_{(-\infty, a]}\left(W_s\right) d W_s+\frac{1}{2} L_t(a).
    \end{align*}
\end{cor}

\begin{rmk}
    For every Borel measurable function $f \colon \R \sto [0,\infty)$,
    \begin{equation*}
        \int_0^t f(W_s)ds = \int_{-\infty}^\infty f(x)L_t(x)dx.
    \end{equation*}
\end{rmk}
\begin{rmk}
    For any semimartingale $X = X_0 + M + A$, we have the similar definition of local time $\Lambda_t$, which satisfies
    \begin{equation*}
        \int_0^t f\left(X_s\right) d\langle M\rangle_s=\int_{-\infty}^{\infty} f(x) \Lambda_t(x) d x, \quad 0 \leq t<\infty,
    \end{equation*}
    and the Tanaka-Meyer formula
    \begin{equation*}
        \left|X_t-a\right|=\left|X_0-a\right|+\int_0^t \operatorname{sign}\left(X_s-a\right) d X_s+\Lambda_t(a).
    \end{equation*}
\end{rmk}





























